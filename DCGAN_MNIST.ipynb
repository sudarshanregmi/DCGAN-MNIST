{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN-MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad5YqMKDjAyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#imports\n",
        "\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import torchvision.datasets as dset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsEFpITbjGF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Allow the optimization \n",
        "cudnn.benchmark = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Knc4-374jn7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imageSize = 64  \n",
        "batchSize = 64\n",
        "nz = 100                # size of the latent z vector\n",
        "ngf = 64                \n",
        "ndf = 64                \n",
        "ngpu = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgQKvC9rjQVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load MNIST dataset\n",
        "\n",
        "dataset = dset.MNIST(root='data/mnist', download=True,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(imageSize),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, ), (0.5, )),\n",
        "                           ]))\n",
        "nc = 1   # number of channels in MNIST dataset is 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4otCMiojhya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batchSize,\n",
        "                                         shuffle=True, num_workers=2) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApTMkZFLj5Jr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLPhLVFFkcbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_weights(net):\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            m.weight.data.normal_(0, 0.02)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.ConvTranspose2d):\n",
        "            m.weight.data.normal_(0, 0.02)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            m.weight.data.normal_(1, 0.02)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.zero_()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoK1wFbcnD2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(nz, ngf*8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf*8),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(ngf*8, ngf*4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf*4),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf*2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        initialize_weights(self)\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.is_cuda and self.ngpu > 1:\n",
        "            output = nn.parallel.data_parallel(self.main, input, range(self.gpu))\n",
        "        else:\n",
        "            output = self.main(input)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgdzgwgCs26l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.is_cuda and self.ngpu > 1:\n",
        "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
        "        else:\n",
        "            output = self.main(input)\n",
        "\n",
        "        return output.view(-1, 1).squeeze(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEnZWQOIurXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "netG = Generator(ngpu).to(device)\n",
        "netD = Discriminator(ngpu).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNq3cRvP9LHu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "f27396a8-faee-4d35-cc27-1a2e55621f90"
      },
      "source": [
        "print(netG)\n",
        "print(netD)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generator(\n",
            "  (main): Sequential(\n",
            "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (13): Tanh()\n",
            "  )\n",
            ")\n",
            "Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (12): Sigmoid()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfSr4TbyFCGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCELoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLMWp2J3qLRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fixed_noise = torch.randn(batchSize, nz, 1, 1, device=device)\n",
        "fake_label = 0\n",
        "real_label = 1\n",
        "\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizerG= optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HicUW55dFHv6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05de123b-7e4f-4439-ddbd-140515d98b30"
      },
      "source": [
        "for epoch in range(20):\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "\n",
        "        netD.zero_grad()\n",
        "        trainData = data[0].to(device)\n",
        "        batch_size = trainData.size(0)\n",
        "        label = torch.full((batch_size,), real_label, device=device)\n",
        "\n",
        "        output = netD(trainData)\n",
        "        D_err_real = criterion(output, label)\n",
        "        D_err_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "\n",
        "        #train with fake data\n",
        "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "        fake = netG(noise)\n",
        "        label.fill_(fake_label)\n",
        "        output = netD(fake.detach())\n",
        "        D_err_fake = criterion(output, label)\n",
        "        D_err_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        D_err = D_err_real + D_err_fake\n",
        "        optimizerD.step()\n",
        "\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)\n",
        "        output = netD(fake)\n",
        "        G_err = criterion(output, label)\n",
        "        G_err.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        optimizerG.step()\n",
        "\n",
        "        print('Epoch: ', epoch, ' Iteration: ', i, 'Generator loss: ', G_err.item(),\n",
        "              ' Discriminator loss ', D_err.item()  )\n",
        "        if i % 100 == 0:\n",
        "            vutils.save_image(trainData, 'trainImage.png', normalize=True)\n",
        "            fake = netG(fixed_noise)\n",
        "            vutils.save_image(fake.detach(), 'fakeImage.png', normalize=True)\n",
        "\n",
        "    torch.save(netG.state_dict(), 'netG_torch_%d.pth' % epoch)\n",
        "    torch.save(netD.state_dict(), 'netD_torch_%d.pth' % epoch)    "
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  19  Iteration:  901 Generator loss:  61.445045471191406  Discriminator loss  1.4901164746561335e-08\n",
            "Epoch:  19  Iteration:  902 Generator loss:  61.62031555175781  Discriminator loss  0.0\n",
            "Epoch:  19  Iteration:  903 Generator loss:  61.19973373413086  Discriminator loss  1.4901162970204496e-08\n",
            "Epoch:  19  Iteration:  904 Generator loss:  61.49208068847656  Discriminator loss  1.862645371275562e-09\n",
            "Epoch:  19  Iteration:  905 Generator loss:  61.446372985839844  Discriminator loss  1.862645371275562e-09\n",
            "Epoch:  19  Iteration:  906 Generator loss:  62.05714416503906  Discriminator loss  1.862645371275562e-09\n",
            "Epoch:  19  Iteration:  907 Generator loss:  61.31952667236328  Discriminator loss  7.450581485102248e-09\n",
            "Epoch:  19  Iteration:  908 Generator loss:  61.46104431152344  Discriminator loss  1.3038517820973539e-08\n",
            "Epoch:  19  Iteration:  909 Generator loss:  61.27679443359375  Discriminator loss  5.587936335871291e-09\n",
            "Epoch:  19  Iteration:  910 Generator loss:  61.140899658203125  Discriminator loss  7.450581485102248e-09\n",
            "Epoch:  19  Iteration:  911 Generator loss:  61.01476287841797  Discriminator loss  5.587936335871291e-09\n",
            "Epoch:  19  Iteration:  912 Generator loss:  60.46651077270508  Discriminator loss  0.0\n",
            "Epoch:  19  Iteration:  913 Generator loss:  61.636474609375  Discriminator loss  3.725290742551124e-09\n",
            "Epoch:  19  Iteration:  914 Generator loss:  61.36293411254883  Discriminator loss  1.862645371275562e-09\n",
            "Epoch:  19  Iteration:  915 Generator loss:  61.6632194519043  Discriminator loss  1.1175872671742582e-08\n",
            "Epoch:  19  Iteration:  916 Generator loss:  61.14518737792969  Discriminator loss  1.1175873559921001e-08\n",
            "Epoch:  19  Iteration:  917 Generator loss:  61.614356994628906  Discriminator loss  1.4901162970204496e-08\n",
            "Epoch:  19  Iteration:  918 Generator loss:  61.20868682861328  Discriminator loss  0.0\n",
            "Epoch:  19  Iteration:  919 Generator loss:  60.884483337402344  Discriminator loss  1.4901162970204496e-08\n",
            "Epoch:  19  Iteration:  920 Generator loss:  61.68352508544922  Discriminator loss  0.0\n",
            "Epoch:  19  Iteration:  921 Generator loss:  61.40603256225586  Discriminator loss  5.587936335871291e-09\n",
            "Epoch:  19  Iteration:  922 Generator loss:  61.40149688720703  Discriminator loss  0.0\n",
            "Epoch:  19  Iteration:  923 Generator loss:  61.52772521972656  Discriminator loss  0.0\n",
            "Epoch:  19  Iteration:  924 Generator loss:  61.873931884765625  Discriminator loss  3.911556945013217e-08\n",
            "Epoch:  19  Iteration:  925 Generator loss:  61.296630859375  Discriminator loss  0.0\n",
            "Epoch:  19  Iteration:  926 Generator loss:  60.790771484375  Discriminator loss  1.862645371275562e-09\n",
            "Epoch:  19  Iteration:  927 Generator loss:  61.24718475341797  Discriminator loss  1.862645371275562e-09\n",
            "Epoch:  19  Iteration:  928 Generator loss:  61.45429992675781  Discriminator loss  3.725290742551124e-09\n",
            "Epoch:  19  Iteration:  929 Generator loss:  61.321739196777344  Discriminator loss  1.862645371275562e-09\n",
            "Epoch:  19  Iteration:  930 Generator loss:  61.77801513671875  Discriminator loss  0.0\n",
            "Epoch:  19  Iteration:  931 Generator loss:  61.29296875  Discriminator loss  3.725290742551124e-09\n",
            "Epoch:  19  Iteration:  932 Generator loss:  61.518653869628906  Discriminator loss  1.750895108898476e-07\n",
            "Epoch:  19  Iteration:  933 Generator loss:  61.54423522949219  Discriminator loss  3.725290742551124e-09\n",
            "Epoch:  19  Iteration:  934 Generator loss:  61.562557220458984  Discriminator loss  1.862645371275562e-09\n",
            "Epoch:  19  Iteration:  935 Generator loss:  61.40684509277344  Discriminator loss  0.0\n",
            "Epoch:  19  Iteration:  936 Generator loss:  61.11497497558594  Discriminator loss  1.3038517820973539e-08\n",
            "Epoch:  19  Iteration:  937 Generator loss:  62.1710319519043  Discriminator loss  1.862645504502325e-08\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}